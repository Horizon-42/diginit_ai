{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "%pip install transformers gradio torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848fd8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained conversational model and tokenizer\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624420b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(message, history):\n",
    "    \"\"\"\n",
    "    Generate a response from the model.\n",
    "    Note: For a real legal/insurance bot, we would inject context or use a RAG pipeline here.\n",
    "    \"\"\"\n",
    "    # Encode the user input\n",
    "    new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # Generate a response\n",
    "    # We limit max_length to keep it quick for the demo\n",
    "    bot_input_ids = new_user_input_ids\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, \n",
    "        max_length=1000, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "    # Decode the response (skip the input tokens)\n",
    "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "    if not response:\n",
    "        return \"I'm not sure what to say to that.\"\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdddda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and launch the Gradio Chat Interface\n",
    "demo = gr.ChatInterface(\n",
    "    predict,\n",
    "    title=\"Dignit AI Model Demo\",\n",
    "    description=\"A demo of the conversational model. Ask about general topics (Note: This base model is not yet fine-tuned for legal advice).\",\n",
    "    examples=[\"Hello, how are you?\", \"What is the process for asylum?\", \"Tell me about insurance.\"]\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
